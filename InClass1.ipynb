{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import CostFuncs as cf\n",
    "\n",
    "class Example1(cf.SecondOrderCostFunc):\n",
    "    def f(self,x):\n",
    "        self.num_f_calls += 1\n",
    "        return x[0]**2 + x[1]**2 - x[0]*x[1] + 2.0*x[0] - 4.0*x[1] + 10.0\n",
    "    \n",
    "    def df(self,x):\n",
    "        self.num_df_calls +=1\n",
    "        return np.array([2.0*x[0]- x[1] + 2, 2.0*x[1] - x[0] - 4])\n",
    "    \n",
    "    def d2f(self,x):\n",
    "        self.num_d2f_calls +=1\n",
    "        return np.array([[2.0, -1.0],[-1, 2.0]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Gradient Descent Approach\n",
    "## In-class example\n",
    "The in-class example was $$x^2 + y^2 - xy +2x -4y +10$$.  We had started to do gradient descent.  Let's see if we can finish that example. The cost function class is above (`Example1`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 is\t\t[0. 0.]\tcost is\t10.0\tgradient is\t[ 2. -4.]\n",
      "x1 is\t\t[-0.89442719  1.78885438]\tcost is\t6.655728090000841\tgradient is\t[-1.57770876  0.47213595]\n",
      "x2 is\t\t[-0.17591016  1.57383515]\tcost is\t6.137594137068957\tgradient is\t[ 0.07434454 -0.67641954]\n",
      "x3 is\t\t[-0.20663701  1.85340164]\tcost is\t6.033897284961888\tgradient is\t[-0.26667565 -0.08655971]\n",
      "x4 is\t\t[-0.00600397  1.91852471]\tcost is\t6.006185094829956\tgradient is\t[ 0.06946734 -0.1569466 ]\n",
      "x5 is\t\t[-0.03801978  1.99085759]\tcost is\t6.001181495076336\tgradient is\t[-0.06689715  0.01973496]\n",
      "x6 is\t\t[-0.00956888  1.98246445]\tcost is\t6.000231263523917\tgradient is\t[-0.00160221 -0.02550223]\n",
      "x7 is\t\t[-0.0088714   1.99356621]\tcost is\t6.000063018643402\tgradient is\t[-0.01130901 -0.00399617]\n",
      "x8 is\t\t[-1.00531223e-03  1.99634579e+00]\tcost is\t6.000010690292098\tgradient is\t[ 0.00164359 -0.00630311]\n",
      "x9 is\t\t[-1.79470539e-03  1.99937309e+00]\tcost is\t6.000002488864284\tgradient is\t[-0.0029625   0.00054088]\n",
      "Final result is  [-6.40585339e-04  1.99916237e+00]  with cost of  6.000000575396378  and gradient of  [-0.00044354 -0.00103467]\n",
      "I called f() 49 times and df() 11 times\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-6.40585339e-04,  1.99916237e+00]), 6.000000575396378)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradDescent(my_f : cf.DerivCostFunc, x_init : np.array, max_iters=10) -> (np.array, float):\n",
    "    curr_x = x_init\n",
    "    curr_iter=0\n",
    "    best_alpha = 8.0 / 1.5\n",
    "    while curr_iter < max_iters:\n",
    "        curr_cost = my_f.f(curr_x)\n",
    "        curr_d = my_f.df(curr_x)\n",
    "        print('x',str(curr_iter),' is\\t\\t',curr_x,'\\tcost is\\t',str(curr_cost),'\\tgradient is\\t',curr_d,sep='')\n",
    "        #Search in the steepest descent direction\n",
    "        curr_alpha = best_alpha * 1.5\n",
    "        best_alpha = curr_alpha\n",
    "        direction = -curr_d / np.linalg.norm(curr_d)\n",
    "        best_cost = my_f.f(curr_x + curr_alpha*direction)\n",
    "        test_cost = best_cost\n",
    "        while best_cost == test_cost:\n",
    "            curr_alpha = curr_alpha / 2.0\n",
    "            test_cost = my_f.f(curr_x + curr_alpha * direction)\n",
    "            #print('  alpha: ',curr_alpha,'\\ttest_cost:\\t',test_cost)\n",
    "            if test_cost < best_cost:\n",
    "                best_cost = test_cost\n",
    "                best_alpha = curr_alpha\n",
    "                #print('Setting best_alpha to ',best_alpha)\n",
    "        curr_x += direction * best_alpha\n",
    "        curr_iter +=1\n",
    "    print('Final result is ',curr_x,' with cost of ',my_f.f(curr_x),' and gradient of ',my_f.df(curr_x))\n",
    "    print('I called f()',my_f.num_f_calls,'times and df()',my_f.num_df_calls,'times')\n",
    "    return (curr_x, my_f.f(curr_x))\n",
    "\n",
    "gradDescent(Example1(), np.array([0.0, 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let me also try the \"default\" cost parameter \n",
    "This lets me test my gradient descent and test the default cost function as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 is\t\t[[-9.2521543   6.39003063]]\tcost is\t11.244325264075346\tgradient is\t[[-0.82282877  0.56828938]]\n",
      "x1 is\t\t[[-2.66952416  1.84371559]]\tcost is\t3.2443252640753464\tgradient is\t[[-0.82282877  0.56828938]]\n",
      "x2 is\t\t[[-0.20103786  0.13884745]]\tcost is\t0.24432526407534622\tgradient is\t[[-0.82282877  0.56828938]]\n",
      "x3 is\t\t[[ 0.03038273 -0.02098394]]\tcost is\t0.0369247359246538\tgradient is\t[[ 0.82282877 -0.56828938]]\n",
      "x4 is\t\t[[ 0.00868705 -0.00599974]]\tcost is\t0.010557548424653798\tgradient is\t[[ 0.82282877 -0.56828938]]\n",
      "x5 is\t\t[[ 0.00055117 -0.00038067]]\tcost is\t0.000669853112153798\tgradient is\t[[ 0.82282877 -0.56828938]]\n",
      "x6 is\t\t[[ 0.00016981 -0.00011728]]\tcost is\t0.00020636739438036045\tgradient is\t[[ 0.82282877 -0.56828938]]\n",
      "x7 is\t\t[[ 2.67915106e-05 -1.85036444e-05]]\tcost is\t3.256025021532139e-05\tgradient is\t[[ 0.82282877 -0.56828938]]\n",
      "x8 is\t\t[[-2.35241113e-08  1.62470045e-08]]\tcost is\t2.8589315623441524e-08\tgradient is\t[[-0.82282877  0.56828938]]\n",
      "x9 is\t\t[[-3.88419337e-09  2.68263086e-09]]\tcost is\t4.720536670112796e-09\tgradient is\t[[-0.82282877  0.56828938]]\n",
      "Final result is  [[-2.01708751e-10  1.39310809e-10]]  with cost of  2.4514061636365836e-10  and gradient of  [[-0.82282877  0.56828938]]\n",
      "I called f() 67 times and df() 11 times\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-2.01708751e-10,  1.39310809e-10]]), 2.4514061636365836e-10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default = cf.DerivCostFunc()\n",
    "gradDescent(default, np.random.randn(1,2)*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton Step ... class example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_x is  [0. 0.]\n",
      "curr_x is  [0. 2.]  with a cost of  6.0\n",
      "curr_x is  [0. 2.]  with a cost of  6.0\n",
      "curr_x is  [0. 2.]  with a cost of  6.0\n",
      "curr_x is  [0. 2.]  with a cost of  6.0\n",
      "curr_x is  [0. 2.]  with a cost of  6.0\n",
      "curr_x is  [0. 2.]  with a cost of  6.0\n",
      "curr_x is  [0. 2.]  with a cost of  6.0\n",
      "curr_x is  [0. 2.]  with a cost of  6.0\n",
      "curr_x is  [0. 2.]  with a cost of  6.0\n",
      "curr_x is  [0. 2.]  with a cost of  6.0\n",
      "I called f()  10  times\n"
     ]
    }
   ],
   "source": [
    "def newtonOpt(my_f : cf.SecondOrderCostFunc, init_x : np.array, max_iters = 10) -> (np.array, float):\n",
    "    curr_x = init_x\n",
    "    print('curr_x is ',curr_x)\n",
    "    for i in range(max_iters):\n",
    "        curr_deriv = my_f.df(curr_x)\n",
    "        curr_hess = my_f.d2f(curr_x)\n",
    "        step = np.linalg.solve(curr_hess, curr_deriv)\n",
    "        curr_x -= step\n",
    "        print('curr_x is ',curr_x,' with a cost of ',my_f.f(curr_x))\n",
    "    print('I called f() ',my_f.num_f_calls,' times')\n",
    "    return curr_x, my_f.f(curr_x)\n",
    "        \n",
    "last_x, cost = newtonOpt(Example1(), np.array([0.0,0.0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFGS\n",
    "First, I will need to come up with a cost function that does not converge in one step...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPF(cf.DerivCostFunc): #Goldstein-Price Function from Wikipedia\n",
    "    def f(self,x):\n",
    "        self.num_f_calls += 1\n",
    "        subterm11 = (19-14*x[0]+3*x[0]*x[0] - 14*x[1] + 6*x[0]*x[1] + 3 * x[1]*x[1])\n",
    "        subterm21 = (18-32*x[0]+12*x[0]*x[0]+48*x[1]-36*x[0]*x[1]+27*x[1]*x[1])\n",
    "        term1 = 1 + ((x[0] + x[1] + 1)**2)*subterm11\n",
    "        term2 = 30 + ((2*x[0] - 3*x[1])**2)*subterm21\n",
    "        return term1 * term2\n",
    "    \n",
    "    def df(self,x):\n",
    "        self.num_df_calls +=1\n",
    "        subterm11 = (19-14*x[0]+3*x[0]*x[0] - 14*x[1] + 6*x[0]*x[1] + 3 * x[1]*x[1])\n",
    "        subterm21 = (18-32*x[0]+12*x[0]*x[0]+48*x[1]-36*x[0]*x[1]+27*x[1]*x[1])\n",
    "        term1 = 1 + ((x[0] + x[1] + 1)**2)*subterm11\n",
    "        term2 = 30 + ((2*x[0] - 3*x[1])**2)*subterm21\n",
    "        dterm1x = 2*(x[0] + x[1] + 1)*subterm11 + ((x[0]+x[1]+1)**2)*(-14+6*x[0]+6*x[1])\n",
    "        dterm2x = 4*(2*x[0]-3*x[1])*subterm21 + ((2*x[0] - 3*x[1])**2)*(-32 + 24*x[0] - 36*x[1])\n",
    "        dterm1y = 2*(x[0]+x[1]+1)*subterm11 + ((x[0]+x[1]+1)**2)*(-14 + 6*x[0] + 6*x[1])\n",
    "        dterm2y = -6*(2*x[0]-3*x[1])*subterm21 + ((2*x[0] - 3*x[1])**2)*(48 -36*x[0] + 54*x[1])\n",
    "        return np.array([dterm1x*term2 + term1*dterm2x, dterm1y*term2 + term1*dterm2y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFGS(my_f : cf.DerivCostFunc, init_x : np.array, init_B = None) -> (np.array, float):\n",
    "    size_x = init_x.shape\n",
    "    if size_x[0] == 1:\n",
    "        N = size_x[1]\n",
    "    else:\n",
    "        N = size_x[0]\n",
    "    if init_B is None:\n",
    "        init_B = np.eye(N)\n",
    "    curr_B = init_B.copy()\n",
    "    curr_H = np.linalg.inv(curr_B)\n",
    "    curr_x = init_x.copy()\n",
    "    c1_wolfe = .1\n",
    "    c2_wolfe = .5\n",
    "    next_deriv = my_f.df(curr_x)\n",
    "    curr_iter = 0\n",
    "    while curr_iter < 10:\n",
    "        curr_cost = my_f.f(curr_x)\n",
    "        curr_deriv = next_deriv\n",
    "        print('curr_x is ',curr_x,'with cost of ',curr_cost)\n",
    "        print('curr_H is ',curr_H, 'curr_deriv is',curr_deriv)\n",
    "        direction = -np.dot(curr_H,curr_deriv)\n",
    "        mag_change = np.dot(direction,curr_deriv)\n",
    "        print('Direction is ',direction)\n",
    "        #Do a line search here...  Use the Wolfe conditions to ensure BFGS keeps things p.s.d.\n",
    "        curr_alpha = 1\n",
    "        cont_iter = True\n",
    "        while cont_iter:\n",
    "            #Evaluate at current alpha\n",
    "            next_cost = my_f.f(curr_x+direction*curr_alpha)\n",
    "            #print('alpha of ',curr_alpha,'gives cost of',next_cost)\n",
    "            if (next_cost-curr_cost) <= curr_alpha * mag_change * c1_wolfe: #First Wolfe condition...\n",
    "                new_deriv = my_f.df(curr_x+direction*curr_alpha)\n",
    "                if np.dot(new_deriv,direction) >= (c2_wolfe * mag_change):\n",
    "                    cont_iter = False\n",
    "                    curr_alpha *= 2\n",
    "            curr_alpha *= 0.5\n",
    "        #print('Terminated loop with alpha of ',curr_alpha)\n",
    "        movement = direction * curr_alpha #s_k in book (6.5)\n",
    "        next_x = curr_x + movement\n",
    "        next_deriv = my_f.df(next_x)\n",
    "        diff_deriv = next_deriv-curr_deriv #y_k in book, (6.5)\n",
    "        rho = 1/np.dot(diff_deriv, movement)\n",
    "        mult = (np.eye(N) - rho * np.outer(movement, diff_deriv))\n",
    "        #print('To do BFGS update, mult is ',mult,' movement is',movement,'rho is ',rho)\n",
    "        next_H = np.dot(np.dot(mult,curr_H),mult.T) + rho * np.outer(movement,movement)\n",
    "        #get ready for next iteration\n",
    "        curr_deriv = next_deriv\n",
    "        curr_x = next_x\n",
    "        curr_H = next_H\n",
    "        curr_iter += 1\n",
    "    print ('curr_x of ',curr_x,'cost of ',my_f.f(curr_x))\n",
    "    return curr_x, my_f.f(curr_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_x is  [0. 0.] with cost of  10.0\n",
      "curr_H is  [[1. 0.]\n",
      " [0. 1.]] curr_deriv is [ 2. -4.]\n",
      "Direction is  [-2.  4.]\n",
      "curr_x is  [-1.  2.] with cost of  7.0\n",
      "curr_H is  [[0.70918367 0.36734694]\n",
      " [0.36734694 0.69387755]] curr_deriv is [-2.  1.]\n",
      "Direction is  [1.05102041 0.04081633]\n",
      "curr_x is  [0.05102041 2.04081633] with cost of  6.002186588921282\n",
      "curr_H is  [[0.67231962 0.3453533 ]\n",
      " [0.3453533  0.6922249 ]] curr_deriv is [0.06122449 0.03061224]\n",
      "Direction is  [-0.05173447 -0.04233464]\n",
      "curr_x is  [-7.14057203e-04  1.99848169e+00] with cost of  6.000001730985319\n",
      "curr_H is  [[0.66801875 0.33082357]\n",
      " [0.33082357 0.67132534]] curr_deriv is [ 9.01966993e-05 -2.32256501e-03]\n",
      "Direction is  [0.00070811 0.00152936]\n",
      "curr_x is  [-5.95104181e-06  2.00001105e+00] with cost of  6.000000000223177\n",
      "curr_H is  [[0.6715571  0.33356873]\n",
      " [0.33356873 0.666678  ]] curr_deriv is [-2.29485337e-05  2.80439420e-05]\n",
      "Direction is  [ 6.05666864e-06 -1.10413658e-05]\n",
      "curr_x is  [1.05626829e-07 2.00000001e+00] with cost of  6.000000000000011\n",
      "curr_H is  [[0.6691125  0.3353459 ]\n",
      " [0.3353459  0.66832272]] curr_deriv is [ 2.06169371e-07 -9.54582546e-08]\n",
      "Direction is  [-1.05938968e-07 -5.34113294e-09]\n",
      "curr_x is  [-3.12138901e-10  2.00000000e+00] with cost of  6.0\n",
      "curr_H is  [[0.66699204 0.33403882]\n",
      " [0.33403882 0.66819631]] curr_deriv is [-3.67432307e-10 -2.01552552e-10]\n",
      "Direction is  [3.12400802e-10 2.57413327e-10]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-87ddfa65ee64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBFGS\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mExample1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-17ec121a850c>\u001b[0m in \u001b[0;36mBFGS\u001b[1;34m(my_f, init_x, init_B)\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnext_cost\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mcurr_cost\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcurr_alpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmag_change\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mc1_wolfe\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#First Wolfe condition...\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0mnew_deriv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurr_x\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdirection\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcurr_alpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_deriv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdirection\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mc2_wolfe\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmag_change\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m                     \u001b[0mcont_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                     \u001b[0mcurr_alpha\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sol,cost = BFGS (Example1(), np.array([0.0,0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
